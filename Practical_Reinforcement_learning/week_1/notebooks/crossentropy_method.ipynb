{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEEdFNCiBunF"
   },
   "source": [
    "# Crossentropy method\n",
    "\n",
    "This notebook will teach you to solve reinforcement learning problems with crossentropy method. We'll follow-up by scaling everything up and using neural network policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1103,
     "status": "ok",
     "timestamp": 1603102737428,
     "user": {
      "displayName": "Antoine Pouillaude",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjnah9HD2oHdjlCYKi-hlp5zzqpxNWZPSHbsDbPjQ=s64",
      "userId": "16158663870014642555"
     },
     "user_tz": -120
    },
    "id": "K9t-RWtpBunK"
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
    "\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/grading.py -O ../grading.py\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week1_intro/submit.py\n",
    "\n",
    "    !touch .setup_complete\n",
    "\n",
    "# This code creates a virtual display to draw game images on.\n",
    "# It will have no effect if your machine has a monitor.\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "executionInfo": {
     "elapsed": 15566,
     "status": "ok",
     "timestamp": 1603097782609,
     "user": {
      "displayName": "Antoine Pouillaude",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjnah9HD2oHdjlCYKi-hlp5zzqpxNWZPSHbsDbPjQ=s64",
      "userId": "16158663870014642555"
     },
     "user_tz": -120
    },
    "id": "F4HbUxd2Buna",
    "outputId": "8544341e-ec48-4af5-d9b3-67158e1c6127"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 1055,
     "status": "ok",
     "timestamp": 1603097923169,
     "user": {
      "displayName": "Antoine Pouillaude",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjnah9HD2oHdjlCYKi-hlp5zzqpxNWZPSHbsDbPjQ=s64",
      "userId": "16158663870014642555"
     },
     "user_tz": -120
    },
    "id": "gkIGB7MpBunt",
    "outputId": "4f58a676-24f8-4b74-f0f1-d048e9a31ee1"
   },
   "outputs": [],
   "source": [
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"n_states=%i, n_actions=%i\" % (n_states, n_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Af-Nj6WBun2"
   },
   "source": [
    "# Create stochastic policy\n",
    "\n",
    "This time our policy should be a probability distribution.\n",
    "\n",
    "```policy[s,a] = P(take action a | in state s)```\n",
    "\n",
    "Since we still use integer state and action representations, you can use a 2-dimensional array to represent the policy.\n",
    "\n",
    "Please initialize the policy __uniformly__, that is, probabililities of all actions should be equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 531,
     "status": "ok",
     "timestamp": 1603103026194,
     "user": {
      "displayName": "Antoine Pouillaude",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjnah9HD2oHdjlCYKi-hlp5zzqpxNWZPSHbsDbPjQ=s64",
      "userId": "16158663870014642555"
     },
     "user_tz": -120
    },
    "id": "famJsXhvBun4"
   },
   "outputs": [],
   "source": [
    "def initialize_policy(n_states, n_actions):\n",
    "    policy = np.array([[1. / n_actions for _ in range(n_actions)] for _ in range(n_states)])\n",
    "    \n",
    "    return policy\n",
    "\n",
    "policy = initialize_policy(n_states, n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 558,
     "status": "ok",
     "timestamp": 1603103028011,
     "user": {
      "displayName": "Antoine Pouillaude",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjnah9HD2oHdjlCYKi-hlp5zzqpxNWZPSHbsDbPjQ=s64",
      "userId": "16158663870014642555"
     },
     "user_tz": -120
    },
    "id": "VSfJnNHyBuoD"
   },
   "outputs": [],
   "source": [
    "assert type(policy) in (np.ndarray, np.matrix)\n",
    "assert np.allclose(policy, 1./n_actions)\n",
    "assert np.allclose(np.sum(policy, axis=1), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sK0pijfxBuoM"
   },
   "source": [
    "# Play the game\n",
    "\n",
    "Just like before, but we also record all states and actions we took."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 538,
     "status": "ok",
     "timestamp": 1603103577911,
     "user": {
      "displayName": "Antoine Pouillaude",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjnah9HD2oHdjlCYKi-hlp5zzqpxNWZPSHbsDbPjQ=s64",
      "userId": "16158663870014642555"
     },
     "user_tz": -120
    },
    "id": "0yQ7oCQYBuoO"
   },
   "outputs": [],
   "source": [
    "def generate_session(env, policy, t_max=10**4):\n",
    "    \"\"\"\n",
    "    Play game until end or for t_max ticks.\n",
    "    :param policy: an array of shape [n_states,n_actions] with action probabilities\n",
    "    :returns: list of states, list of actions and sum of rewards\n",
    "    \"\"\"\n",
    "    states, actions = [], []\n",
    "    total_reward = 0.\n",
    "\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        # Hint: you can use np.random.choice for sampling action\n",
    "        # https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html\n",
    "        a_sp = list(range(len(policy[s])))\n",
    "        a = np.random.choice(a_sp, size=None, replace=True, p=policy[s])\n",
    "\n",
    "        new_s, r, done, info = env.step(a)\n",
    "\n",
    "        # Record information we just got from the environment.\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_reward += r\n",
    "\n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return states, actions, total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "executionInfo": {
     "elapsed": 414,
     "status": "ok",
     "timestamp": 1603103578867,
     "user": {
      "displayName": "Antoine Pouillaude",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjnah9HD2oHdjlCYKi-hlp5zzqpxNWZPSHbsDbPjQ=s64",
      "userId": "16158663870014642555"
     },
     "user_tz": -120
    },
    "id": "6BBDcGvoBuoX",
    "outputId": "71715077-158d-4887-bb94-d3bcd6453d94"
   },
   "outputs": [],
   "source": [
    "s, a, r = generate_session(env, policy)\n",
    "assert type(s) == type(a) == list\n",
    "assert len(s) == len(a)\n",
    "assert type(r) in [float, np.float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "executionInfo": {
     "elapsed": 2511,
     "status": "ok",
     "timestamp": 1603103600794,
     "user": {
      "displayName": "Antoine Pouillaude",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjnah9HD2oHdjlCYKi-hlp5zzqpxNWZPSHbsDbPjQ=s64",
      "userId": "16158663870014642555"
     },
     "user_tz": -120
    },
    "id": "vJnnn5BKBuox",
    "outputId": "dec035b1-595a-4859-84d1-f3614e644a31"
   },
   "outputs": [],
   "source": [
    "# let's see the initial reward distribution\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sample_rewards = [generate_session(env, policy, t_max=1000)[-1] for _ in range(200)]\n",
    "\n",
    "plt.hist(sample_rewards, bins=20)\n",
    "plt.vlines([np.percentile(sample_rewards, 50)], [0], [100], label=\"50'th percentile\", color='green')\n",
    "plt.vlines([np.percentile(sample_rewards, 90)], [0], [100], label=\"90'th percentile\", color='red')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WeIeYgA8BupL"
   },
   "source": [
    "### Crossentropy method steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 539,
     "status": "ok",
     "timestamp": 1603109985190,
     "user": {
      "displayName": "Antoine Pouillaude",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjnah9HD2oHdjlCYKi-hlp5zzqpxNWZPSHbsDbPjQ=s64",
      "userId": "16158663870014642555"
     },
     "user_tz": -120
    },
    "id": "S3CmMgh8BupM"
   },
   "outputs": [],
   "source": [
    "def select_elites(states_batch, actions_batch, rewards_batch, percentile):\n",
    "    \"\"\"\n",
    "    Select states and actions from games that have rewards >= percentile\n",
    "    :param states_batch: list of lists of states, states_batch[session_i][t]\n",
    "    :param actions_batch: list of lists of actions, actions_batch[session_i][t]\n",
    "    :param rewards_batch: list of rewards, rewards_batch[session_i]\n",
    "\n",
    "    :returns: elite_states,elite_actions, both 1D lists of states and respective actions from elite sessions\n",
    "\n",
    "    Please return elite states and actions in their original order \n",
    "    [i.e. sorted by session number and timestep within session]\n",
    "\n",
    "    If you are confused, see examples below. Please don't assume that states are integers\n",
    "    (they will become different later).\n",
    "    \"\"\"\n",
    "\n",
    "    reward_threshold = np.percentile(rewards_batch, percentile)\n",
    "    elite_rewards = [i for i, r in enumerate(rewards_batch) if r >= reward_threshold]\n",
    "\n",
    "    elite_states = [s for i in elite_rewards for s in states_batch[i]]\n",
    "    elite_actions = [a for i in elite_rewards for a in actions_batch[i]]\n",
    "\n",
    "    return elite_states, elite_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 554,
     "status": "ok",
     "timestamp": 1603109992867,
     "user": {
      "displayName": "Antoine Pouillaude",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjnah9HD2oHdjlCYKi-hlp5zzqpxNWZPSHbsDbPjQ=s64",
      "userId": "16158663870014642555"
     },
     "user_tz": -120
    },
    "id": "IgUrMa0nBupb",
    "outputId": "af9278d4-4de2-4cdd-b01b-e79df5f8f0ca"
   },
   "outputs": [],
   "source": [
    "states_batch = [\n",
    "    [1, 2, 3],     # game1\n",
    "    [4, 2, 0, 2],  # game2\n",
    "    [3, 1],        # game3\n",
    "]\n",
    "\n",
    "actions_batch = [\n",
    "    [0, 2, 4],     # game1\n",
    "    [3, 2, 0, 1],  # game2\n",
    "    [3, 3],        # game3\n",
    "]\n",
    "rewards_batch = [\n",
    "    3,  # game1\n",
    "    4,  # game2\n",
    "    5,  # game3\n",
    "]\n",
    "\n",
    "test_result_0 = select_elites(states_batch, actions_batch, rewards_batch, percentile=0)\n",
    "test_result_30 = select_elites(states_batch, actions_batch, rewards_batch, percentile=30)\n",
    "test_result_90 = select_elites(states_batch, actions_batch, rewards_batch, percentile=90)\n",
    "test_result_100 = select_elites(states_batch, actions_batch, rewards_batch, percentile=100)\n",
    "\n",
    "assert np.all(test_result_0[0] == [1, 2, 3, 4, 2, 0, 2, 3, 1])  \\\n",
    "    and np.all(test_result_0[1] == [0, 2, 4, 3, 2, 0, 1, 3, 3]), \\\n",
    "    \"For percentile 0 you should return all states and actions in chronological order\"\n",
    "assert np.all(test_result_30[0] == [4, 2, 0, 2, 3, 1]) and \\\n",
    "    np.all(test_result_30[1] == [3, 2, 0, 1, 3, 3]), \\\n",
    "    \"For percentile 30 you should only select states/actions from two first\"\n",
    "assert np.all(test_result_90[0] == [3, 1]) and \\\n",
    "    np.all(test_result_90[1] == [3, 3]), \\\n",
    "    \"For percentile 90 you should only select states/actions from one game\"\n",
    "assert np.all(test_result_100[0] == [3, 1]) and\\\n",
    "    np.all(test_result_100[1] == [3, 3]), \\\n",
    "    \"Please make sure you use >=, not >. Also double-check how you compute percentile.\"\n",
    "\n",
    "print(\"Ok!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 559,
     "status": "ok",
     "timestamp": 1603115200818,
     "user": {
      "displayName": "Antoine Pouillaude",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjnah9HD2oHdjlCYKi-hlp5zzqpxNWZPSHbsDbPjQ=s64",
      "userId": "16158663870014642555"
     },
     "user_tz": -120
    },
    "id": "wCk7ZlyuBupp"
   },
   "outputs": [],
   "source": [
    "def update_policy(elite_states, elite_actions):\n",
    "    \"\"\"\n",
    "    Given old policy and a list of elite states/actions from select_elites,\n",
    "    return new updated policy where each action probability is proportional to\n",
    "\n",
    "    policy[s_i,a_i] ~ #[occurences of si and ai in elite states/actions]\n",
    "\n",
    "    Don't forget to normalize policy to get valid probabilities and handle 0/0 case.\n",
    "    In case you never visited a state, set probabilities for all actions to 1./n_actions\n",
    "\n",
    "    :param elite_states: 1D list of states from elite sessions\n",
    "    :param elite_actions: 1D list of actions from elite sessions\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    new_policy = np.zeros([n_states, n_actions])\n",
    "\n",
    "    for s, a in list(zip(elite_states, elite_actions)):\n",
    "        new_policy[s][a] += 1\n",
    "    row_sum = np.sum(new_policy, axis=1)\n",
    "    zero_idx = [i for i, x in enumerate(row_sum) if x == 0]\n",
    "    new_policy[zero_idx] = [1.] * n_actions\n",
    "    row_sum = np.sum(new_policy, axis=1)\n",
    "    new_policy = new_policy / row_sum[:, np.newaxis]\n",
    "\n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 671,
     "status": "ok",
     "timestamp": 1603115203360,
     "user": {
      "displayName": "Antoine Pouillaude",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjnah9HD2oHdjlCYKi-hlp5zzqpxNWZPSHbsDbPjQ=s64",
      "userId": "16158663870014642555"
     },
     "user_tz": -120
    },
    "id": "yblkHlIoBurO",
    "outputId": "16b695bf-86e6-4af1-a2e6-09070b584506"
   },
   "outputs": [],
   "source": [
    "elite_states = [1, 2, 3, 4, 2, 0, 2, 3, 1]\n",
    "elite_actions = [0, 2, 4, 3, 2, 0, 1, 3, 3]\n",
    "\n",
    "new_policy = update_policy(elite_states, elite_actions)\n",
    "\n",
    "assert np.isfinite(new_policy).all(), \\\n",
    "    \"Your new policy contains NaNs or +-inf. Make sure you don't divide by zero.\"\n",
    "assert np.all(new_policy >= 0), \\\n",
    "    \"Your new policy can't have negative action probabilities\"\n",
    "assert np.allclose(new_policy.sum(axis=-1), 1), \\\n",
    "    \"Your new policy should be a valid probability distribution over actions\"\n",
    "\n",
    "reference_answer = np.array([\n",
    "    [1.,  0.,  0.,  0.,  0.],\n",
    "    [0.5,  0.,  0.,  0.5,  0.],\n",
    "    [0.,  0.33333333,  0.66666667,  0.,  0.],\n",
    "    [0.,  0.,  0.,  0.5,  0.5]])\n",
    "assert np.allclose(new_policy[:4, :5], reference_answer)\n",
    "\n",
    "print(\"Ok!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m9H-jRjTBurX"
   },
   "source": [
    "# Training loop\n",
    "Generate sessions, select N best and fit to those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 637,
     "status": "ok",
     "timestamp": 1603115228810,
     "user": {
      "displayName": "Antoine Pouillaude",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjnah9HD2oHdjlCYKi-hlp5zzqpxNWZPSHbsDbPjQ=s64",
      "userId": "16158663870014642555"
     },
     "user_tz": -120
    },
    "id": "ut_DZjUfBurZ"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def show_progress(rewards_batch, log, percentile, reward_range=[-990, +10]):\n",
    "    \"\"\"\n",
    "    A convenience function that displays training progress. \n",
    "    No cool math here, just charts.\n",
    "    \"\"\"\n",
    "\n",
    "    mean_reward = np.mean(rewards_batch)\n",
    "    threshold = np.percentile(rewards_batch, percentile)\n",
    "    log.append([mean_reward, threshold])\n",
    "    \n",
    "    plt.figure(figsize=[8, 4])\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(list(zip(*log))[0], label='Mean rewards')\n",
    "    plt.plot(list(zip(*log))[1], label='Reward thresholds')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(rewards_batch, range=reward_range)\n",
    "    plt.vlines([np.percentile(rewards_batch, percentile)],\n",
    "               [0], [100], label=\"percentile\", color='red')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    clear_output(True)\n",
    "    print(\"mean reward = %.3f, threshold=%.3f\" % (mean_reward, threshold))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 553,
     "status": "ok",
     "timestamp": 1603115231042,
     "user": {
      "displayName": "Antoine Pouillaude",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjnah9HD2oHdjlCYKi-hlp5zzqpxNWZPSHbsDbPjQ=s64",
      "userId": "16158663870014642555"
     },
     "user_tz": -120
    },
    "id": "cess9NUXBurg"
   },
   "outputs": [],
   "source": [
    "# reset policy just in case\n",
    "policy = initialize_policy(n_states, n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "executionInfo": {
     "elapsed": 90120,
     "status": "ok",
     "timestamp": 1603115608429,
     "user": {
      "displayName": "Antoine Pouillaude",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjnah9HD2oHdjlCYKi-hlp5zzqpxNWZPSHbsDbPjQ=s64",
      "userId": "16158663870014642555"
     },
     "user_tz": -120
    },
    "id": "T4jlw7JJBurn",
    "outputId": "3420dbb2-a510-4c14-c7db-b82abd79f32f"
   },
   "outputs": [],
   "source": [
    "n_sessions = 250     # sample this many sessions\n",
    "percentile = 50      # take this percent of session with highest rewards\n",
    "learning_rate = 0.5  # how quickly the policy is updated, on a scale from 0 to 1\n",
    "\n",
    "log = []\n",
    "\n",
    "for i in range(100):\n",
    "    %time sessions = [generate_session(env, policy, t_max=1000) for _ in range(n_sessions)]\n",
    "\n",
    "    states_batch, actions_batch, rewards_batch = zip(*sessions)\n",
    "\n",
    "    elite_states, elite_actions = select_elites(states_batch,actions_batch, rewards_batch, percentile)\n",
    "\n",
    "    new_policy = update_policy(elite_states, elite_actions)\n",
    "\n",
    "    policy = learning_rate * new_policy + (1 - learning_rate) * policy\n",
    "\n",
    "    # display results on chart\n",
    "    show_progress(rewards_batch, log, percentile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9zXaCwchBurv"
   },
   "source": [
    "### Reflecting on results\n",
    "\n",
    "You may have noticed that the taxi problem quickly converges from less than -1000 to a near-optimal score and then descends back into -50/-100. This is in part because the environment has some innate randomness. Namely, the starting points of passenger/driver change from episode to episode.\n",
    "\n",
    "In case CEM failed to learn how to win from one distinct starting point, it will simply discard it because no sessions from that starting point will make it into the \"elites\".\n",
    "\n",
    "To mitigate that problem, you can either reduce the threshold for elite sessions (duct tape way) or change the way you evaluate strategy (theoretically correct way). For each starting state, you can sample an action randomly, and then evaluate this action by running _several_ games starting from it and averaging the total reward. Choosing elite sessions with this kind of sampling (where each session's reward is counted as the average of the rewards of all sessions with the same starting state and action) should improve the performance of your policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qgQzfuFQKF4p"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of crossentropy_method.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/yandexdataschool/Practical_RL/blob/coursera/week1_intro/crossentropy_method.ipynb",
     "timestamp": 1603097015591
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
